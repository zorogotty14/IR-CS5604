{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45b5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6184ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textClean(text):\n",
    "    \"\"\"\n",
    "    Get rid of the non-letter and non-number characters\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    text = textClean(text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "def constructLabeledSentences(data):\n",
    "    sentences = []\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def getEmbeddings(path,vector_dimension=300):\n",
    "    \"\"\"\n",
    "    Generate Doc2Vec training and testing data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    missing_rows = []\n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
    "            missing_rows.append(i)\n",
    "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
    "\n",
    "    x = constructLabeledSentences(data['text'])\n",
    "    y = data['label'].values\n",
    "\n",
    "    text_model = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, workers=7, epochs=10,\n",
    "                         seed=1)\n",
    "    text_model.build_vocab(x)\n",
    "    text_model.train(x, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
    "\n",
    "    train_size = int(0.8 * len(x))\n",
    "    test_size = len(x) - train_size\n",
    "\n",
    "    text_train_arrays = np.zeros((train_size, vector_dimension))\n",
    "    text_test_arrays = np.zeros((test_size, vector_dimension))\n",
    "    train_labels = np.zeros(train_size)\n",
    "    test_labels = np.zeros(test_size)\n",
    "\n",
    "    for i in range(train_size):\n",
    "        text_train_arrays[i] = text_model.docvecs['Text_' + str(i)]\n",
    "        train_labels[i] = y[i]\n",
    "\n",
    "    j = 0\n",
    "    for i in range(train_size, train_size + test_size):\n",
    "        text_test_arrays[j] = text_model.docvecs['Text_' + str(i)]\n",
    "        test_labels[j] = y[i]\n",
    "        j = j + 1\n",
    "\n",
    "    return text_train_arrays, text_test_arrays, train_labels, test_labels\n",
    "\n",
    "\n",
    "def clean_data():\n",
    "    \"\"\"\n",
    "    Generate processed string\n",
    "    \"\"\"\n",
    "    path = 'C://Users//gauth//Desktop//courses//CS5604//project//dataset1//combined_data.csv'\n",
    "    vector_dimension=300\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    missing_rows = []\n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
    "            missing_rows.append(i)\n",
    "    data = data.drop(missing_rows).reset_index()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    x = data.loc[:,'text'].values\n",
    "    y = data.loc[:,'label'].values\n",
    "\n",
    "    train_size = int(0.8 * len(y))\n",
    "    test_size = len(x) - train_size\n",
    "\n",
    "    xtr = x[:train_size]\n",
    "    xte = x[train_size:]\n",
    "    ytr = y[:train_size]\n",
    "    yte = y[train_size:]\n",
    "\n",
    "    np.save('xtr_shuffled.npy',xtr)\n",
    "    np.save('xte_shuffled.npy',xte)\n",
    "    np.save('ytr_shuffled.npy',ytr)\n",
    "    np.save('yte_shuffled.npy',yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ecb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot.plotters as skplt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb081fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Read the text data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./xtr_shuffled.npy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./xte_shuffled.npy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./ytr_shuffled.npy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./yte_shuffled.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m     clean_data()\n\u001b[0;32m     18\u001b[0m xtr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./xtr_shuffled.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m xte \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./xte_shuffled.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m, in \u001b[0;36mclean_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(missing_rows)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m---> 86\u001b[0m     data\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cleanup(data\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     88\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1795\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1888\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1885\u001b[0m \n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m   1887\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[1;32m-> 1888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_column(loc, value, pi)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1993\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1990\u001b[0m     \u001b[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m     \u001b[38;5;66;03m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mcolumn_setitem(loc, plane_indexer, value)\n\u001b[1;32m-> 1993\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   1994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_iset_item(loc, value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4271\u001b[0m, in \u001b[0;36mDataFrame._clear_item_cache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m klass(values, name\u001b[38;5;241m=\u001b[39mname, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   4268\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;66;03m# Lookup Caching\u001b[39;00m\n\u001b[1;32m-> 4271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clear_item_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_item_cache\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m   4274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_item_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "def plot_cmat(yte, ypred):\n",
    "    '''Plotting confusion matrix'''\n",
    "    skplt.plot_confusion_matrix(yte, ypred)\n",
    "    plt.show()\n",
    "\n",
    "# Read the text data\n",
    "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
    "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./yte_shuffled.npy'):\n",
    "    clean_data()\n",
    "\n",
    "\n",
    "xtr = np.load('./xtr_shuffled.npy', allow_pickle=True)\n",
    "xte = np.load('./xte_shuffled.npy', allow_pickle=True)\n",
    "y_train = np.load('./ytr_shuffled.npy', allow_pickle=True)\n",
    "y_test = np.load('./yte_shuffled.npy', allow_pickle=True)\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3437b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
