{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b1b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea76bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title    source  \\\n",
      "0                 What is Really Fueling Voter Anger  dailykos   \n",
      "1  Brown's victory in Mass. senate race hardly a ...       wpo   \n",
      "2  E2 Round-up: What Brown means for climate bill...      hill   \n",
      "3  Video: Brown camp files criminal complaint aga...       wat   \n",
      "4          There Are No Safe Seats for Democrats Now       fox   \n",
      "\n",
      "                                                text        bias  \n",
      "0  voterâ€™s real anger. They are all highlighting ...        Left  \n",
      "1  While many are describing the election to fill...   Lean Left  \n",
      "2  All eyes are on Massachusetts today, the site ...      Center  \n",
      "3  Dan Winslow, Brown campaign counsel, spoke to ...  Lean Right  \n",
      "4  Scott Brown's stunning upset victory in Massac...       Right  \n"
     ]
    }
   ],
   "source": [
    "def json_to_dataframe(file_path):\n",
    "    rows = []\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # The prefix argument to ijson.items() specifies the path to the sequence of items in the JSON document\n",
    "        for items in ijson.items(file, 'item'):\n",
    "            # Process each list of items\n",
    "            for item in items:  # Assuming each 'item' is a list of dictionaries\n",
    "                if isinstance(item, dict):  # Ensure that each element in the list is a dictionary\n",
    "                    title = item.get('title', '')\n",
    "                    source = item.get('source', '')\n",
    "                    # Retrieve 'text' field, check if it's a list, and join correctly\n",
    "                    text_content = item.get('text', '')\n",
    "                    if isinstance(text_content, list):  # Check if the 'text' field is a list\n",
    "                        text_content = ''.join(text_content)\n",
    "                    bias = item.get('bias', '')\n",
    "\n",
    "                    # Append to rows list as a dictionary\n",
    "                    rows.append({\n",
    "                        'title': title,\n",
    "                        'source': source,\n",
    "                        'text': text_content,\n",
    "                        'bias': bias\n",
    "                    })\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'C://Users//gauth//Desktop//courses//CS5604//project//dataset1//BIGNEWSALIGN_min.json'\n",
    "data = json_to_dataframe(file_path)\n",
    "print(data.head())  # Print the first few rows to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053819f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a CSV file\n",
    "csv_file_path = 'C://Users//gauth//Desktop//courses//CS5604//project//dataset1//combined_data2.csv'  # Specify your file path\n",
    "data.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44292a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textClean(text):\n",
    "    \"\"\"\n",
    "    Get rid of the non-letter and non-number characters\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    text = textClean(text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "def constructLabeledSentences(data):\n",
    "    sentences = []\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def getEmbeddings(path,vector_dimension=300):\n",
    "    \"\"\"\n",
    "    Generate Doc2Vec training and testing data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    missing_rows = []\n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
    "            missing_rows.append(i)\n",
    "    data = data.drop(missing_rows).reset_index().drop(['index','id','source'],axis=1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
    "\n",
    "    x = constructLabeledSentences(data['text'])\n",
    "    y = data['bias'].values\n",
    "\n",
    "    text_model = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, workers=7, epochs=10,\n",
    "                         seed=1)\n",
    "    text_model.build_vocab(x)\n",
    "    text_model.train(x, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
    "\n",
    "    train_size = int(0.8 * len(x))\n",
    "    test_size = len(x) - train_size\n",
    "\n",
    "    text_train_arrays = np.zeros((train_size, vector_dimension))\n",
    "    text_test_arrays = np.zeros((test_size, vector_dimension))\n",
    "    train_labels = np.zeros(train_size)\n",
    "    test_labels = np.zeros(test_size)\n",
    "\n",
    "    for i in range(train_size):\n",
    "        text_train_arrays[i] = text_model.docvecs['Text_' + str(i)]\n",
    "        train_labels[i] = y[i]\n",
    "\n",
    "    j = 0\n",
    "    for i in range(train_size, train_size + test_size):\n",
    "        text_test_arrays[j] = text_model.docvecs['Text_' + str(i)]\n",
    "        test_labels[j] = y[i]\n",
    "        j = j + 1\n",
    "\n",
    "    return text_train_arrays, text_test_arrays, train_labels, test_labels\n",
    "\n",
    "\n",
    "def clean_data():\n",
    "    \"\"\"\n",
    "    Generate processed string\n",
    "    \"\"\"\n",
    "    path = 'C:/Users/gauth/Desktop/courses/CS5604/project/dataset1/combined_data2.csv'\n",
    "    vector_dimension=300\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    x = data.loc[:,'text'].values\n",
    "    y = data.loc[:,'bias'].values\n",
    "\n",
    "    train_size = int(0.8 * len(y))\n",
    "    test_size = len(x) - train_size\n",
    "\n",
    "    xtr = x[:train_size]\n",
    "    xte = x[train_size:]\n",
    "    ytr = y[:train_size]\n",
    "    yte = y[train_size:]\n",
    "\n",
    "    np.save('xtr_min.npy',xtr)\n",
    "    np.save('xte_min.npy',xte)\n",
    "    np.save('ytr_min.npy',ytr)\n",
    "    np.save('yte_min.npy',yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b18ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38614ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr = np.load('./xtr_min.npy', allow_pickle=True)\n",
    "xte = np.load('./xte_min.npy', allow_pickle=True)\n",
    "y_train = np.load('./ytr_min.npy', allow_pickle=True)\n",
    "y_test = np.load('./yte_min.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a429558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot.plotters as skplt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cf17704",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc35566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Random Training Samples:\n",
      "         0   1    2     3    4     5     6   7     8    9  ...   491  492  \\\n",
      "134105  48  29  391  3553  141  1386  1449  85  1957  114  ...    29  109   \n",
      "89954    0   0    0     0    0     0     0   0     0    0  ...   337  178   \n",
      "25851    0   0    0     0    0     0     0   0     0    0  ...  4009   21   \n",
      "972      0   0    0     0    0     0     0   0     0    0  ...   176  926   \n",
      "20102    0   0    0     0    0     0     0   0     0    0  ...     1  147   \n",
      "\n",
      "         493   494   495   496   497   498   499       Bias  \n",
      "134105   392   222  2145   654   399  2552   507       Left  \n",
      "89954    511  1028   592   310  2179  4344     7  Lean Left  \n",
      "25851    360   122  1847  2543   459   230  2240  Lean Left  \n",
      "972      214   290   333  1852  1036  3988    29      Right  \n",
      "20102   1101   351  2244    30   774  2679  2180  Lean Left  \n",
      "\n",
      "[5 rows x 501 columns]\n",
      "\n",
      "5 Random Test Samples:\n",
      "       0    1  2   3   4     5     6   7    8    9  ...  491  492   493   494  \\\n",
      "27152  0    0  0   0   0     0     0   0    0    0  ...   44   79  2552   104   \n",
      "6401   3  401  8  34  12  4346  3107  49  431  758  ...  232   66  2351  3116   \n",
      "24457  0    0  0   0   0     0     0   0    0    0  ...  817    8  2751  3394   \n",
      "16333  0    0  0   0   0     0     0   0    0    0  ...    1  214     3    65   \n",
      "23935  0    0  0   0   0     0     0   0    0    0  ...   35   34     4   838   \n",
      "\n",
      "        495   496   497   498   499        Bias  \n",
      "27152   119    92  4756  1824  1158   Lean Left  \n",
      "6401    471  2913    30   329    74      Center  \n",
      "24457   504   413   129   866     1      Center  \n",
      "16333    26  1164   392  1503   189  Lean Right  \n",
      "23935  1891  1441   319   438  4180        Left  \n",
      "\n",
      "[5 rows x 501 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert arrays to DataFrames if they aren't already (if needed)\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "y_train_df = pd.DataFrame(y_train, columns=['Bias'])\n",
    "y_test_df = pd.DataFrame(y_test, columns=['Bias'])\n",
    "\n",
    "# Print 5 random samples from each DataFrame\n",
    "print(\"5 Random Training Samples:\")\n",
    "print(pd.concat([X_train_df, y_train_df], axis=1).sample(5))\n",
    "print()\n",
    "\n",
    "print(\"5 Random Test Samples:\")\n",
    "print(pd.concat([X_test_df, y_test_df], axis=1).sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd50726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded y_train: [0 2 4 ... 4 1 4]\n",
      "Encoded y_test: [4 1 1 ... 1 4 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the training data and transform both training and test data\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert encoded labels to numpy arrays (if they aren't already)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Encoded y_train:\", y_train)  # Print first 5 encoded labels from training set\n",
    "print(\"Encoded y_test:\", y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f4f39ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (137796,)\n",
      "Shape of y_test: (34449,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13820803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (137796, 500)\n",
      "Shape of y_test: (34449, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_train:\", X_train.shape)\n",
    "print(\"Shape of y_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2d799c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6d4b0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Training completed in 12303.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauth\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SVM classifier with verbose output and a maximum iteration limit\n",
    "svm_classifier = SVC(kernel='rbf', class_weight='balanced', decision_function_shape='ovo', verbose=True, max_iter=200)\n",
    "\n",
    "# Monitor training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total duration\n",
    "duration = end_time - start_time\n",
    "print(f\"Training completed in {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dd1cee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model: 0.20276350547185695\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      6873\n",
      "           1       1.00      0.00      0.00      6850\n",
      "           2       0.20      1.00      0.34      6970\n",
      "           3       0.40      0.00      0.00      6919\n",
      "           4       0.00      0.00      0.00      6837\n",
      "\n",
      "    accuracy                           0.20     34449\n",
      "   macro avg       0.32      0.20      0.07     34449\n",
      "weighted avg       0.32      0.20      0.07     34449\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0    0 6873    0    0]\n",
      " [   0   11 6836    3    0]\n",
      " [   0    0 6970    0    0]\n",
      " [   0    0 6915    4    0]\n",
      " [   0    0 6834    3    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of SVM model:\", accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "123b08fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to svm_classifier.joblib\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "# Save the model to a file\n",
    "model_filename = 'svm_classifier.joblib'\n",
    "dump(svm_classifier, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d94e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
