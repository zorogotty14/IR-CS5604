{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2aa466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f079cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title    source  \\\n",
      "0  IL-SCT: White Doesn't Have To Sign Burris Cert...  dailykos   \n",
      "1             Defiant Burris Says Senate Seat Is His       nyt   \n",
      "2  Burris Wins at Ill. High Court; Senate Path St...      hill   \n",
      "3                   Support grows for Burris on Hill       wat   \n",
      "4  Illinois High Court: Burris Appointment Valid ...       fox   \n",
      "\n",
      "                                                text        bias  \n",
      "0  Remember what I said yesterday about the Senat...        Left  \n",
      "1  CHICAGO Even as Senate leaders continued to ch...   Lean Left  \n",
      "2  Aspiring Sen. Roland Burris (D) won his certif...      Center  \n",
      "3  Democratic leaders backpedaled furiously Wedne...  Lean Right  \n",
      "4  The appointment of Roland Burris to the U.S. S...       Right  \n"
     ]
    }
   ],
   "source": [
    "def json_to_dataframe(file_path):\n",
    "    rows = []\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # The prefix argument to ijson.items() specifies the path to the sequence of items in the JSON document\n",
    "        for items in ijson.items(file, 'item'):\n",
    "            # Process each list of items\n",
    "            for item in items:  # Assuming each 'item' is a list of dictionaries\n",
    "                if isinstance(item, dict):  # Ensure that each element in the list is a dictionary\n",
    "                    title = item.get('title', '')\n",
    "                    source = item.get('source', '')\n",
    "                    # Retrieve 'text' field, check if it's a list, and join correctly\n",
    "                    text_content = item.get('text', '')\n",
    "                    if isinstance(text_content, list):  # Check if the 'text' field is a list\n",
    "                        text_content = ''.join(text_content)\n",
    "                    bias = item.get('bias', '')\n",
    "\n",
    "                    # Append to rows list as a dictionary\n",
    "                    rows.append({\n",
    "                        'title': title,\n",
    "                        'source': source,\n",
    "                        'text': text_content,\n",
    "                        'bias': bias\n",
    "                    })\n",
    "\n",
    "    # Convert list of rows to DataFrame\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'C://Users//gauth//Desktop//courses//CS5604//project//dataset1//BIGNEWSALIGN_minimized_balanced_labeled_text_combined.json'\n",
    "data = json_to_dataframe(file_path)\n",
    "print(data.head())  # Print the first few rows to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b9cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a CSV file\n",
    "csv_file_path = 'C://Users//gauth//Desktop//courses//CS5604//project//dataset1//combined_data1.csv'  # Specify your file path\n",
    "data.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "404bb1f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     58\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC://Users//gauth//Desktop//courses//CS5604//project//dataset1//combined_data1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 59\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m getEmbeddings(path)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Save processed data\u001b[39;00m\n\u001b[0;32m     62\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, x_train)\n",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m, in \u001b[0;36mgetEmbeddings\u001b[1;34m(path, vector_dimension)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetEmbeddings\u001b[39m(path, vector_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    Generate Doc2Vec training and testing data\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Clean and truncate text data\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(cleanup)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1233\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1246\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1444\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim import utils\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def textClean(text):\n",
    "    \"\"\"\n",
    "    Get rid of the non-letter and non-number characters\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    return text\n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "    Clean and truncate text to 1000 words\n",
    "    \"\"\"\n",
    "    text = textClean(text)\n",
    "    text = \" \".join(text[:1000])  # Join the first 1000 words only\n",
    "    return text\n",
    "\n",
    "def constructTaggedDocuments(data):\n",
    "    \"\"\"\n",
    "    Construct TaggedDocument objects required for Doc2Vec\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for index, text in enumerate(data):\n",
    "        sentences.append(TaggedDocument(utils.to_unicode(text).split(), ['Text_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "def getEmbeddings(path, vector_dimension=300):\n",
    "    \"\"\"\n",
    "    Generate Doc2Vec training and testing data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    # Clean and truncate text data\n",
    "    data['text'] = data['text'].apply(cleanup)\n",
    "\n",
    "    x = constructTaggedDocuments(data['text'])\n",
    "    y = data['bias'].values\n",
    "\n",
    "    model = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, workers=7, epochs=10, seed=1)\n",
    "    model.build_vocab(x)\n",
    "    model.train(x, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split([model.dv[i] for i in range(len(x))], y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "# Example usage:\n",
    "path = 'C://Users//gauth//Desktop//courses//CS5604//project//dataset1//combined_data1.csv'\n",
    "x_train, x_test, y_train, y_test = getEmbeddings(path)\n",
    "\n",
    "# Save processed data\n",
    "np.save('x_train.npy', x_train)\n",
    "np.save('x_test.npy', x_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8fa0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot.plotters as skplt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80669cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "def plot_cmat(yte, ypred):\n",
    "    '''Plotting confusion matrix'''\n",
    "    skplt.plot_confusion_matrix(yte, ypred)\n",
    "    plt.show()\n",
    "\n",
    "# Read the text data\n",
    "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
    "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./yte_shuffled.npy'):\n",
    "    clean_data()\n",
    "\n",
    "\n",
    "xtr = np.load('./xtr_shuffled.npy', allow_pickle=True)\n",
    "xte = np.load('./xte_shuffled.npy', allow_pickle=True)\n",
    "y_train = np.load('./ytr_shuffled.npy', allow_pickle=True)\n",
    "y_test = np.load('./yte_shuffled.npy', allow_pickle=True)\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m176/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 248ms/step - accuracy: 0.7379 - loss: 0.5180"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Define constants\n",
    "top_words = 5000\n",
    "embedding_vector_length = 32\n",
    "max_review_length = 500\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Embedding layer without input_length\n",
    "model.add(Embedding(input_dim=top_words + 2, output_dim=embedding_vector_length))\n",
    "\n",
    "# Add the LSTM layer\n",
    "model.add(LSTM(units=100))\n",
    "\n",
    "# Add the Dense output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_num, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6c1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
